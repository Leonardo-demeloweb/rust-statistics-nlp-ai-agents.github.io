

# ğŸ“Š Stat Rust Engine â€” Business Insights Pipeline

Pipeline modular e escalÃ¡vel para ingestÃ£o, anÃ¡lise estatÃ­stica vetorizada e geraÃ§Ã£o de insights estratÃ©gicos utilizando Python, Rust (via WASM) e LLMs locais.

---

## ğŸš€ Arquitetura

```
stat-rust-engine/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                 # FastAPI para exposiÃ§Ã£o dos endpoints
â”œâ”€â”€ crew_agents/
â”‚   â”œâ”€â”€ agents/                 # Agentes inteligentes
â”‚   â”‚   â”œâ”€â”€ data_ingestion_agent.py
â”‚   â”‚   â”œâ”€â”€ query_execution_agent.py
â”‚   â”‚   â”œâ”€â”€ nlp_to_sql_agent.py
â”‚   â”‚   â””â”€â”€ insights_llm_agent.py
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â””â”€â”€ crew_orchestrator.py
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ statistics_wasm_service.py
â”œâ”€â”€ db.py                       # Conector DuckDB
â”œâ”€â”€ rust_module/
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.rs             # ExecuÃ§Ã£o via stdin/stdout (WASI)
â”‚   â”‚   â””â”€â”€ statistics.rs      # LÃ³gica de cÃ¡lculo estatÃ­stico
â”‚   â””â”€â”€ target/
â”‚       â””â”€â”€ wasm32-wasi/
â”‚           â””â”€â”€ release/
â”‚               â””â”€â”€ stat_rust_engine.wasm
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â””â”€â”€ README.md                   # ğŸ“„ VocÃª estÃ¡ aqui
```

---

## ğŸ§© Componentes e Fluxo

1ï¸âƒ£ **Data Ingestion Agent**  
IngestÃ£o de arquivos CSV, registrando dinamicamente tabelas no DuckDB.

2ï¸âƒ£ **Query Execution Agent**  
Executa queries SQL simples e extrai vetores numÃ©ricos.

3ï¸âƒ£ **NLP to SQL Agent (SLM Fine-tuned)**  
Converte perguntas em linguagem natural para consultas SQL simples utilizando o modelo local **Text2SQL-1.5B** fine-tuned:

- **Modelo utilizado:** [`yasserrmd/text2sql-1.5b`](https://ollama.com/yasserrmd/Text2SQL-1.5B)  
- ğŸš« **FunÃ§Ãµes agregadas bloqueadas** (AVG, MIN, MAX, etc.) â†’ cÃ¡lculo sempre delegado ao mÃ³dulo Rust.

4ï¸âƒ£ **Statistics WASM Service**  
Processa vetores utilizando o mÃ³dulo **Rust WASM** nativo, garantindo:

âœ… Performance atÃ© **12x superior ao Pandas**  
âœ… ExecuÃ§Ã£o vetorizada e mÃ­nima latÃªncia  
âœ… BaixÃ­ssimo custo computacional (WASM)

5ï¸âƒ£ **Insights LLM Agent**  
Recebe o output estatÃ­stico e gera insights automÃ¡ticos em linguagem natural utilizando modelo LLM local ou remoto.

---

## ğŸ”¥ Exemplo de Pipeline

```bash
curl -X POST "http://localhost:8000/upload_and_analyze/" \
     -F "file=@/caminho/para/dataset.csv" \
     -F "question=Qual foi o nÃ­vel de Ã¡gua registrado ao longo do tempo?"
```

**Resposta:**
```json
{
  "status": "success",
  "table": "leituras_sensores_csv",
  "query": "SELECT nivel_agua FROM leituras_sensores_csv;",
  "statistics": {
    "min": 23.4,
    "max": 98.7,
    "mean": 56.2,
    "median": 54.9,
    "q1": 45.1,
    "q3": 66.3,
    "std_dev": 12.4,
    "coef_var": 22.1
  },
  "insight": "Os dados indicam uma grande variaÃ§Ã£o no nÃ­vel de Ã¡gua, sugerindo..."
}
```

---

## âš™ï¸ Como rodar localmente

### 1. Instale o ambiente Python

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure o arquivo `.env`

Crie um arquivo `.env`:

```env
OPENAI_API_KEY=your_key_if_needed
```

*(Se estiver usando apenas modelo local nÃ£o precisa dessa key)*

### 3. Instale e rode o Ollama + modelo Text2SQL

```bash
# Instale o Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Inicie o servidor Ollama
ollama serve

# Baixe o modelo Text2SQL
ollama pull yasserrmd/text2sql-1.5b:latest

# Valide o modelo
ollama list
```

---

## ğŸŸ¢ Justificativa Arquitetural â€” DelegaÃ§Ã£o do Processamento EstatÃ­stico

Apesar do DuckDB possuir suporte a funÃ§Ãµes agregadas bÃ¡sicas (`AVG()`, `STDDEV()`, `MAX()`, etc.), optamos por **nÃ£o utilizar funÃ§Ãµes agregadas SQL** neste projeto, por decisÃ£o arquitetural estratÃ©gica:

âœ… Garantir **seguranÃ§a e previsibilidade** â€” Evitando queries maliciosas ou incorretas  
âœ… Separar claramente responsabilidades â€” SQL apenas para projeÃ§Ã£o, filtro e schema discovery  
âœ… Delegar cÃ¡lculos vetorizados ao mÃ³dulo **Rust + WASM** para obter:

- **Performance consistente e mÃ­nima latÃªncia**
- **Capacidade de expansÃ£o para cÃ¡lculos complexos**
- **ExecuÃ§Ã£o controlada e auditÃ¡vel**

### Exemplos de CÃ¡lculos NÃ£o ViÃ¡veis no SQL:
- **Coeficiente de VariaÃ§Ã£o ajustado para outliers**
- **NormalizaÃ§Ã£o vetorial por faixas dinÃ¢micas**
- **EstatÃ­sticas condicionais (ex.: com masking ou censura)**
- **CÃ¡lculos com alto volume vetorizado em Edge Computing**

---

## âœ… Resultados Obtidos

Testes realizados com datasets reais demonstraram:

- ğŸ”¥ **ReduÃ§Ã£o do tempo de anÃ¡lise em atÃ© 12x versus Pandas**
- â±ï¸ **LatÃªncia mÃ­nima em consultas com atÃ© 1 milhÃ£o de registros (<100ms)**
- ğŸ¤– **InterpretaÃ§Ã£o automÃ¡tica e geraÃ§Ã£o de insights via LLM**
- ğŸ”€ **Pipeline modular, escalÃ¡vel e independente de ambiente**
- ğŸŒ **Alta portabilidade (Web, Backend ou Edge)**

---
Aqui estÃ¡ a versÃ£o traduzida para inglÃªs do seu README, mantendo toda a estrutura, clareza e boas prÃ¡ticas de documentaÃ§Ã£o:

---

English version:

# ğŸ“Š Stat Rust Engine â€” Business Insights Pipeline

A modular and scalable pipeline for ingestion, vectorized statistical analysis, and strategic insight generation using Python, Rust (via WASM), and local LLMs.

---

## ğŸš€ Architecture

```
stat-rust-engine/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                 # FastAPI to expose API endpoints
â”œâ”€â”€ crew_agents/
â”‚   â”œâ”€â”€ agents/                 # Intelligent agents
â”‚   â”‚   â”œâ”€â”€ data_ingestion_agent.py
â”‚   â”‚   â”œâ”€â”€ query_execution_agent.py
â”‚   â”‚   â”œâ”€â”€ nlp_to_sql_agent.py
â”‚   â”‚   â””â”€â”€ insights_llm_agent.py
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â””â”€â”€ crew_orchestrator.py
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ statistics_wasm_service.py
â”œâ”€â”€ db.py                       # DuckDB connector
â”œâ”€â”€ rust_module/
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.rs             # Execution via stdin/stdout (WASI)
â”‚   â”‚   â””â”€â”€ statistics.rs      # Statistical calculation logic
â”‚   â””â”€â”€ target/
â”‚       â””â”€â”€ wasm32-wasi/
â”‚           â””â”€â”€ release/
â”‚               â””â”€â”€ stat_rust_engine.wasm
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # ğŸ“„ You are here
```

---

## ğŸ§© Components & Workflow

1ï¸âƒ£ **Data Ingestion Agent**  
Ingests CSV files, dynamically registering tables in DuckDB.

2ï¸âƒ£ **Query Execution Agent**  
Executes simple SQL queries and extracts numerical vectors.

3ï¸âƒ£ **NLP to SQL Agent (Fine-Tuned SLM)**  
Converts natural language questions into simple SQL queries using the local fine-tuned model **Text2SQL-1.5B**:

- **Model used:** [`yasserrmd/text2sql-1.5b`](https://ollama.com/yasserrmd/Text2SQL-1.5B)  
- ğŸš« **Aggregate functions are blocked** (AVG, MIN, MAX, etc.) â†’ all calculations are delegated to the Rust module.

4ï¸âƒ£ **Statistics WASM Service**  
Processes numeric vectors using the native **Rust WASM** module, ensuring:

âœ… Up to **12x better performance than Pandas**  
âœ… Vectorized execution with minimal latency  
âœ… Very low computational cost (WASM)

5ï¸âƒ£ **Insights LLM Agent**  
Receives statistical output and automatically generates strategic insights in natural language using a local or remote LLM model.

---

## ğŸ”¥ Example Pipeline Execution

```bash
curl -X POST "http://localhost:8000/upload_and_analyze/" \
     -F "file=@/path/to/dataset.csv" \
     -F "question=What was the recorded water level over time?"
```

**Response:**
```json
{
  "status": "success",
  "table": "leituras_sensores_csv",
  "query": "SELECT nivel_agua FROM leituras_sensores_csv;",
  "statistics": {
    "min": 23.4,
    "max": 98.7,
    "mean": 56.2,
    "median": 54.9,
    "q1": 45.1,
    "q3": 66.3,
    "std_dev": 12.4,
    "coef_var": 22.1
  },
  "insight": "The data shows a large variation in water level, suggesting..."
}
```

---

## âš™ï¸ How to Run Locally

### 1. Set up the Python environment

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 2. Configure `.env` file

Create a `.env` file:

```env
OPENAI_API_KEY=your_key_if_needed
```

*(If using only local models, this key is optional)*

### 3. Install and run Ollama + Text2SQL model

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama server
ollama serve

# Download the Text2SQL model
ollama pull yasserrmd/text2sql-1.5b:latest

# Validate available models
ollama list
```

---

## ğŸŸ¢ Architectural Justification â€” Statistical Processing Delegation

Although DuckDB supports basic aggregate functions (`AVG()`, `STDDEV()`, `MAX()`, etc.), we chose **not to use aggregate SQL functions** in this project, as part of a deliberate architectural decision:

âœ… Ensure **security and predictability** â€” Preventing malicious or incorrect queries  
âœ… Clear separation of responsibilities â€” SQL is used only for projection, filtering, and schema discovery  
âœ… Delegate vectorized calculations to the **Rust + WASM** module to achieve:

- **Consistent performance and minimal latency**
- **Scalability for complex calculations**
- **Controlled and auditable execution**

### Examples of Calculations Not Suitable for SQL:
- **Adjusted Coefficient of Variation for outliers**
- **Dynamic range-based vector normalization**
- **Conditional statistics (e.g., masking or data censoring)**
- **High-volume vectorized processing in Edge Computing**

---

## âœ… Results Achieved

Tests conducted with real datasets demonstrated:

- ğŸ”¥ **Up to 12x reduction in analysis time compared to Pandas**
- â±ï¸ **Minimal latency on queries with up to 1 million records (<100ms)**
- ğŸ¤– **Automatic interpretation and insight generation via LLM**
- ğŸ”€ **Modular, scalable, and environment-independent pipeline**
- ğŸŒ **High portability (Web, Backend, or Edge)**

